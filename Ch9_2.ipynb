{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch9-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinukki/KJU/blob/master/Ch9_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF5ClcP6nFT_",
        "colab_type": "code",
        "outputId": "7ef635eb-ecb6-489f-ac39-77ec81f0c74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "!pip install tensorflow_gpu==2.0.0-rc1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_gpu==2.0.0-rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/cf/2fc69ba3e59edc8333e2676fa71b40197718dea7dc1282c79955cf6b2acb/tensorflow_gpu-2.0.0rc1-cp36-cp36m-manylinux2010_x86_64.whl (380.5MB)\n",
            "\u001b[K     |████████████████████████████████| 380.5MB 37kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.0.8)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 24.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (0.33.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (0.8.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 36.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.17.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==2.0.0-rc1) (3.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==2.0.0-rc1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow_gpu==2.0.0-rc1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow_gpu==2.0.0-rc1) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow_gpu==2.0.0-rc1) (41.6.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-gpu-2.0.0rc1 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82lC8NJvnMDf",
        "colab_type": "code",
        "outputId": "8f4ab08d-e8b4-457c-9b26-e5cc50c327a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIOGzyMHpWJO",
        "colab_type": "text"
      },
      "source": [
        "## 09-2 순환 신경망을 만들고 텍스트를 분류합니다.\n",
        "##### 텍스트 데이터 변환 방법을 알아보고 앞에서 유도한 경사 하강법 공식으로 순환 신경망 클래스를 구현해 보자.\n",
        "\n",
        "> ### 훈련 세트와 검증 세트를 준비합니다.\n",
        "##### IMDB 데이터 세트는 인터넷 영화 데이터 베이스에서 수집한 영화 리뷰 데이터이다. 순환 신경망으로 이 리뷰들이 긍정적인지 부정적인지 판별해보자. 훈련 세트 25,000개, 테스트 세트 25,000개로 구성되어 있으며 훈련 세트에서 5,000개의 세트를 분리하여 검증 세트로 사용한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuKh0zPmnX-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUqr8fGFnYBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train_all, y_train_all), (x_test, y_test)=imdb.load_data(skip_top=20, num_words=100)\n",
        "# skip_top은 가장 많이 등장한 단어들 중 건너뛸 단어의 개수 지정\n",
        "# num_words는 훈련에 사용할 단어의 개수 지정"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKBvTARVpkZU",
        "colab_type": "code",
        "outputId": "78c3f9f4-0ba1-41a7-8b4c-025c5eacdcb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_train_all.shape, y_train_all.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIug3ctUpkd1",
        "colab_type": "code",
        "outputId": "7cf08bc5-bfee-47fc-8cee-c79e289627ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(x_train_all[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 2, 22, 2, 43, 2, 2, 2, 2, 65, 2, 2, 66, 2, 2, 2, 36, 2, 2, 25, 2, 43, 2, 2, 50, 2, 2, 2, 35, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 39, 2, 2, 2, 2, 2, 2, 38, 2, 2, 2, 2, 50, 2, 2, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 22, 71, 87, 2, 2, 43, 2, 38, 76, 2, 2, 2, 2, 22, 2, 2, 2, 2, 2, 2, 2, 2, 2, 62, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 66, 2, 33, 2, 2, 2, 2, 38, 2, 2, 25, 2, 51, 36, 2, 48, 25, 2, 33, 2, 22, 2, 2, 28, 77, 52, 2, 2, 2, 2, 82, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 36, 71, 43, 2, 2, 26, 2, 2, 46, 2, 2, 2, 2, 2, 2, 88, 2, 2, 2, 2, 98, 32, 2, 56, 26, 2, 2, 2, 2, 2, 2, 2, 22, 21, 2, 2, 26, 2, 2, 2, 30, 2, 2, 51, 36, 28, 2, 92, 25, 2, 2, 2, 65, 2, 38, 2, 88, 2, 2, 2, 2, 2, 2, 2, 2, 32, 2, 2, 2, 2, 2, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXF16T1Ppnu8",
        "colab_type": "text"
      },
      "source": [
        "##### 위는 영단어를 고유한 정수에 일대일 대응한 것으로, BoW(Bag of Word) 혹은 어휘 사전이라 부른다. 눈에 띄는 숫자인 2는 사전에 없는 단어를 의미한다. 가장 많이 등장하는 영단어 20개를 건너뛰고 100개 단어만 선택했기 때문에 사전에 없는 영단어가 많다. 추가로 0과 1은 각각 패딩과 글의 시작을 나타내는 데 사용한다. 이 숫자들을 제외하고 훈련 세트를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGpAE6EppzQ",
        "colab_type": "code",
        "outputId": "5b13e1ec-5659-42ea-995e-acad4eefa69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(x_train_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXS_MnInnYDt",
        "colab_type": "code",
        "outputId": "5b03ea28-63ca-47b2-f350-82f94aa96a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "for i in range(len(x_train_all)):\n",
        "  x_train_all[i]=[w for w in x_train_all[i] if w>2]\n",
        "\n",
        "print(x_train_all[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[22, 43, 65, 66, 36, 25, 43, 50, 35, 39, 38, 50, 22, 22, 71, 87, 43, 38, 76, 22, 62, 66, 33, 38, 25, 51, 36, 48, 25, 33, 22, 28, 77, 52, 82, 36, 71, 43, 26, 46, 88, 98, 32, 56, 26, 22, 21, 26, 30, 51, 36, 28, 92, 25, 65, 38, 88, 32, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp8lSdsRprcE",
        "colab_type": "text"
      },
      "source": [
        "##### 훈련 세트를 영단어로 바꿔보자. 어휘 사전은 get_word_index() 함수로 내려받을 수 있다. 이 함수는 영단어와 정수로 구성된 딕셔너리를 반환한다. 다음은 어휘 사전을 내려받은 다음 딕셔너리 키 값을 movie로 지정하여 값을 출력한 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPnTCczvnYGc",
        "colab_type": "code",
        "outputId": "279b7529-4295-4801-a733-04ee1ef1ab9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_to_index=imdb.get_word_index()\n",
        "word_to_index['movie']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ps11kKhpwYX",
        "colab_type": "text"
      },
      "source": [
        "##### 훈련세트에 있는 정수는 3 이상부터 영단어를 의미하므로 3을 뺀 값을 어휘 사전의 인덱스로 사용해야 한다. 훈련 세트를 영단어로 변환하여 출력해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BNG8lsdpxGU",
        "colab_type": "code",
        "outputId": "bd99df79-49a8-49f9-9b36-cfb5df24b76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(word_to_index['film']) # 훈련 데이터에서 22\n",
        "print(word_to_index['just']) # 훈련 데이터에서 43"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqpLb2dsnYJO",
        "colab_type": "code",
        "outputId": "234e3499-58fe-4dc0-8a2f-ab3b564146cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "index_to_word={word_to_index[k]:k for k in word_to_index}\n",
        "\n",
        "for w in x_train_all[0]:\n",
        "  print(index_to_word[w-3], end=' ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "film just story really they you just there an from so there film film were great just so much film would really at so you what they if you at film have been good also they were just are out because them all up are film but are be what they have don't you story so because all all "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj0785JSp0Cs",
        "colab_type": "text"
      },
      "source": [
        "##### 훈련 세트 입력 데이터는 넘파이 배열이 아닌 파이썬 리스트이다. 각 리뷰들의 길이가 달라 샘플의 길이가 다르기 때문이다. 두 샘플의 길이를 확인하여 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkPJCsmEp2JM",
        "colab_type": "code",
        "outputId": "e2778965-472e-49ac-92d0-1622ed6e7ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(x_train_all[0]), len(x_train_all[1])) # 샘플별 길이가 다름을 확인"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1birxQ4-p2Wj",
        "colab_type": "code",
        "outputId": "4e7595b8-16ad-45d8-b840-bfbe760b80d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(y_train_all[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 0 1 0 0 1 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFAyoK4lqCLs",
        "colab_type": "text"
      },
      "source": [
        "##### 검증 세트를 준비해 보자. 25,000개 훈련 세트 중 5,000개만 분리하여 검증 세이트로 사용하자. 넘파이 premutation() 함수를 이용하여 25,000개의 인덱스를 섞은 후 앞의 20,000개ㅐ는 훈련 세트로, 나머지는 검증 세트로 분리한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5VnEl_vnYME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42) # 난수 생성\n",
        "random_index=np.random.permutation(25000) # 섞은 것을 random.index로 지정\n",
        "\n",
        "x_train=x_train_all[random_index[:20000]]\n",
        "y_train=y_train_all[random_index[:20000]]\n",
        "x_val=x_train_all[random_index[20000:]]\n",
        "y_val=y_train_all[random_index[20000:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLTSrixNqFrE",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "> ### 샘플의 길이 맞추기\n",
        "##### 여기서는 일정 길이가 넘으면 샘플을 잘라버리고 모자라면 0으로 채워 샘플의 길이를 맞추도록 한다. 다음은 샘플의 길이를 7로 맞추기 위해 두 문장을 0으로 채우거나 자른 예이다.\n",
        "\n",
        "![13](https://user-images.githubusercontent.com/52277776/69005789-3e9ecf80-096a-11ea-82ab-063b0e74296d.jpg)\n",
        "\n",
        "##### 길이가 7 이하인 샘플의 왼쪽에 0을 추가했다는 것에 주목하자. 만약 오른쪽에 추가했다면 이후 샘플이 순환 신경망에 주입될 때 0이 마지막에 주입되므로 모델의 성능이 좋지 않을 것이다.\n",
        "\n",
        "##### 텐서플로를 이용해 최대 길이를 100으로 설정하여 길이가 동일한 2개의 넘파이 배열 x_train_seq, x_val_seq를 만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKoownNMnYOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "maxlen=100\n",
        "x_train_seq = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val_seq = sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58FFU_pFqI88",
        "colab_type": "text"
      },
      "source": [
        "##### 훈련 세트의 크기를 확인해보자. 위에서 지정한 값으로 샘플의 길이가 변경되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTrmXjYLqM08",
        "colab_type": "code",
        "outputId": "5c0f4a42-422f-4002-cf07-0b00db137d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_train_seq.shape, x_val_seq.shape) # 20,000개는 훈련 세트, 5,000개는 검증 세트"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 100) (5000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDIfgRS2qOH0",
        "colab_type": "code",
        "outputId": "c84636aa-d7cb-4a19-d04a-4055a9bdb577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "print(x_train_seq[0]) # 샘플 길이 변경한 훈련 세트의 첫 번째 샘플 확인(왼쪽에 0으로 채워짐)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0 35 40 27 28 40 22 83 31 85 45\n",
            " 24 23 31 70 31 76 30 98 32 22 28 51 75 56 30 33 97 53 38 46 53 74 31 35\n",
            " 23 34 22 58]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBwxCNYQqQ50",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "> ### 샘플을 원-핫 인코딩하기\n",
        "##### 훈련 데이터 준비를 위한 마지막 작업은 정수 데이터를 원-핫 인코딩하는 것이다.\n",
        "##### 텐서플로의 to_categorical() 함수를 사용하면 원-핫 인코딩을 손쉽게 처리할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b-JI8tonYRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "x_train_onehot = to_categorical(x_train_seq)\n",
        "x_val_onehot = to_categorical(x_val_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYIUsmH4qTK0",
        "colab_type": "text"
      },
      "source": [
        "##### 변환시킨 후 크기를 확인하여 보자. 20,000개의 샘플이 100차원으로 원-핫 인코딩되었다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyMek3-KqTn2",
        "colab_type": "code",
        "outputId": "f82ef056-16f1-4df1-8b51-d5400bb45a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(x_train_onehot.shape)\n",
        "print(x_train_onehot[0].shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 100, 100)\n",
            "(100, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpS6SFmeqTrM",
        "colab_type": "code",
        "outputId": "9dbea0ba-785f-42e3-e0c3-7b6930f22a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(x_train_onehot.nbytes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "800000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH47kJT5qXTF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "> ### 순환 신경망 클래스 구현하기\n",
        "##### 이제 06장에서 구현했던 MiniBatchNetwork 클래스를 기반으로 순환 신경망을 파이썬으로 직접 구현해 보자.\n",
        "\n",
        "##### *__*init__() 메서드는 은닉층의 개수 대신 셀 개수를 받는다. 그리고 셀에 필요한 가중치 w1h, w1x를 선언한다. 또 타임 스텝을 거슬러 그레디언트를 전파하려면 순환층의 활성화 출력을 모두 가지고 있어야 하므로 변수 h를 선언한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qke3TnPwqX87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def __init__(self, n_cells=10, batch_size=32, lerning_rate=0.1):\n",
        "  self.n_cells = n_cells       #셀 개수\n",
        "  self.batch_size = batch_size # 배치 크기\n",
        "  self.w1h = None              # 은닉 상태에 대한 가중치\n",
        "  self.w1x = None              # 입력에 대한 가중치\n",
        "  self.b1 = None               # 순환층의 절편\n",
        "  self.w2 = None               # 출력층의 가중치\n",
        "  self.b2 = None               # 출력층의 절편\n",
        "  self.h = None                # 순환층의 활성화 출력\n",
        "  self.losses = []             # 훈련 손실\n",
        "  self.val_losses = []         # 검증 손실\n",
        "  self.lr = lerning_rate       #학습률"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K-DcAaMqaUc",
        "colab_type": "text"
      },
      "source": [
        "##### 08장에서 글로럿 초기화 방식으로 가중치를 초기화하며 가중치 초기화의 중요성을 설명하였다. 순환 신경망에서는 직교 행렬 초기화(orthogonal initialization)를 사용한다. 이는 순환 셀에서 은닉 상태를 위한 가중치가 반복해서 곱해질 때 너무 커지거나 작아지지 않도록 만들어준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnP8gLM5qZdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(self, n_features, n_classes):\n",
        "  orth_init = tf.initializers.Orthogonal() # 텐서플로가 제공하는 가중치 초기화 클래스 및 직교행렬 초기화 클래스\n",
        "  glorot_init = tf.initializers.GlorotUniform()\n",
        "\n",
        "  self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # 가중치 값이 초기화된 텐서를 넘파이 배열로 변환하여 가중치 변수에 저장 \n",
        "  self.w1x = glorot_init((n_features, self.n_cells)).numpy()\n",
        "  self.b1 = np.zeros(self.n_cells)\n",
        "  self.w2 = glorot_init((self.n_cells, n_classes)).numpy()\n",
        "  self.b2 = np.zeros(n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96L9Ztz_qco8",
        "colab_type": "text"
      },
      "source": [
        "##### 정방향 계산을 forpass() 메서드에 구현한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp_cV9Psqfb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forpass(self, x):\n",
        "  self.h = [np.zeros((x.shape[0], self.n_cells))] # 은닉 상태 초기화"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ppbc4xqg0F",
        "colab_type": "text"
      },
      "source": [
        "##### 각 타임 스텝의 은닉 상태를 저장하기 위해 변수 h를 초기화 한다. 이때 은닉 상태 크기는 (샘플 개수, 셀 개수)이다. 역전파 과정 진행에서 이전 타임 스텝의 은닉 상태를 사용한다. 첫 번째 타임 스텝의 이전 은닉 상태는 없으므로 변수 h의 첫 번째 요소에 0으로 채워진 배열을 추가한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QX3iK-mqk7m",
        "colab_type": "text"
      },
      "source": [
        "##### 그런 다음 넘파이의 swapaxes() 함수를 사용하여 입력 x의 배치 차원과 타임 스텝 차원을 바꾼다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22WM_rG6qjRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq=np.swapaxes(x,0,1) # 배치 차원과 타임 스텝 차원을 바꾼다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VVmsDkRqmzN",
        "colab_type": "text"
      },
      "source": [
        "##### 입력 x는 여러 개의 샘플이 담긴 미니 배치이다. 정방향 계산을 할 때는 한 샘플의 모든 타임 스텝을 처리하고 그 다음에 샘플을 처리하는 방식이 아니다. 미니 배치 안에 있는 모든 샘플의 첫 번째 타임 스텝을 한번에 처리하고 두 번째 타임 스텝을 한 번에 처리해야 한다. 이를 손쉽게 구현하기 위해 배치 차원과 타임 스텝 차원을 바꾼 것이다.\n",
        "\n",
        "![14](https://user-images.githubusercontent.com/52277776/69005791-42325680-096a-11ea-8d45-d54a08fa7db1.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJDxZ8UKqogc",
        "colab_type": "text"
      },
      "source": [
        "##### 마지막으로 각 샘플의 모든 타임 스텝에 대한 정방향 계산을 수행한다. 셀에서 계산된 은닉 상태는 변수 h에 순서대로 추가된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms_rhkleqo20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 순환층의 선형식을 계산한다.\n",
        "for x in seq:\n",
        "  z1=np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1\n",
        "  h=np.tanh(z1)                    # 활성화 함수 적용\n",
        "  self.h.append(h)                 # 역전파를 위한 은닉 상태 저장\n",
        "  z2=np.dot(h, self.w2) + self.b2  # 출력층의 선형식 계산\n",
        "return z2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URRj8hoLqpjt",
        "colab_type": "text"
      },
      "source": [
        "##### 이제 역방향 계산을 backprop() 메서드로 구현해 보자. 여기서도 이 전과 같이 배치 차원과 타임 스텝 차원을 바꾸었다. err_to_cell 변수에 저장되는 값은 $Z_1$ 에 대하여 손실 함수를 미분한 도함수의 결괏값이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdMju7AOqrXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backprop(self, x, err):\n",
        "  m=len(x) # 샘플 개수\n",
        "\n",
        "  # 출력층의 가중치와 절편에 대한 그레디언트 계산\n",
        "  w2_grad = np.dot(self.h[-1].T, err) / m\n",
        "  b2_grad = np.sum(err) / m\n",
        "  # 배치 차원과 타임 스텝 차원을 바꾼다.\n",
        "  seq = np.swapaxes(x, 0, 1)\n",
        "\n",
        "  w1h_grad = w1x_grad = b1_grad = 0 # 초기 0지정\n",
        "  # 셀 직전까지 그레디언트 계산 \n",
        "  err_to_cell = np.dot(err, self.w2.T)*(1-self.h[-1]**2)\n",
        "  # 모든 타임 스텝 거슬러 가며 그레디언트 전파\n",
        "  # 열 번째 까지 진행, h의 마지막 항목은 err_to_cell 정의에 사용하였으므로 제외.\n",
        "  # [A:B:C]코드는 indexA부터 indexB까지 C간격으로 만드는 코드\n",
        "  for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
        "    w1h_grad += np.dot(h.T, err_to_cell)\n",
        "    w1x_grad += np.dot(x.T, err_to_cell)\n",
        "    b1_grad += np.sum(err_to_cell, axis=0) # 행 q셀 직전까지 그레디언트 계산 \n",
        "      err_to_cell = np.dot(err, self.w2.T)*(1-self.h[-1]**2)\n",
        "        # 모든 타임 스텝 거슬러 가며 그레디언트 전파\n",
        "          # 열 번째 까지 진행, h의 마지막 항목은 err_to_cell 정의에 사용하였으므로 제외.\n",
        "            # [A:B:C]코드는 indexA부터 indexB까지 C간격으로 만드는 코드\n",
        "              for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
        "                  w1h_grad += np.dot(h.T, err_to_cell)\n",
        "                      w1x_grad += np.dot(x.T, err_to_cell)\n",
        "                          b1_grad += np.sum(err_to_cell, 합을 구함\n",
        "    #이전 타임 스텝의 셀 직전까지 그레디언트 계산\n",
        "    err_to_cell = np.dot(err_to_cell, self.w1h)*(1-h**2)\n",
        "\n",
        "  w1h_grad /= m\n",
        "  w1x_grad /= m\n",
        "  b1_grad /= map\n",
        "  \n",
        "  return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlbas0Aqr41",
        "colab_type": "text"
      },
      "source": [
        "##### 딱 10개의 타임 스텝만 거슬러 진행한 이유는 무엇일까? 순환 신경망은 타임 스텝을 거슬러 올라가며 그레디언트를 전파할 때 동일한 가중치를 반복적으로 곱한다. 이로 인해 그레디언트가 너무 커지거나 작아지는 문제가 발생하기 쉽다. 이를 방지하기 위해 그레디언트를 전파하는 타임 스텝의 수를 제한해야 하는데, 이를 TBPTT(Truncated Backpropagation Through Time)라고 부른다. 그 다음 for문에서는 $W_{1h}$의 그레디언트(w1h_grad)를 구하기 위해 $Z_1$에 대한 손실함수의 미분값(err_to_cell)에 다음 식을 곱한다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=H_{p}+H_{pp}W_{1h}\\odot(1-H_{p}^2)+H_{ppp}W_{1h}\\odot(1-H_{p}^2)\\odot W_{1h}\\odot(1-H_{pp}^2)+\\dots$$<br>\n",
        "\n",
        "##### 타임 스텝이 거슬러 가며 진행될 때마다 err_to_cell에 $W_{1h}\\odot(1-H_{p}^2)$ 형태를 반복해서 곱함으로써 w1h_grad를 구하는 식을 $H$와 err_to_cell 만의 곱으로 단순화시켰다. 마찬가지로 $W_{1x}$의 그레디언트(w1x_grad)와 $b_1$의 그레디언트(b1_grad)도 누적된 err_to_cell 변수를 사용하여 손쉽게 계산 가능하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTRjhWx_qu4W",
        "colab_type": "text"
      },
      "source": [
        "##### 순환 신경망, RecurrentNetwork 클래스의 전체 코드는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TzG64CdnYTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RecurrentNetwork:\n",
        "\n",
        "  def __init__(self, n_cells=10, batch_size=32, learning_rate=0.1):\n",
        "    self.n_cells = n_cells       #셀 개수\n",
        "    self.batch_size = batch_size # 배치 크기\n",
        "    self.w1h = None              # 은닉 상태에 대한 가중치\n",
        "    self.w1x = None              # 입력에 대한 가중치\n",
        "    self.b1 = None               # 순환층의 절편\n",
        "    self.w2 = None               # 출력층의 가중치\n",
        "    self.b2 = None               # 출력층의 절편\n",
        "    self.h = None                # 순환층의 활성화 출력\n",
        "    self.losses = []             # 훈련 손실\n",
        "    self.val_losses = []         # 검증 손실\n",
        "    self.lr = learning_rate       #학습률\n",
        "\n",
        "  def forpass(self, x):\n",
        "    self.h = [np.zeros((x.shape[0], self.n_cells))] # 은닉 상태 초기화\n",
        "    seq = np.swapaxes(x, 0, 1) # 배치 차원과 타임 스텝 차원을 바꾼다.\n",
        "    # 순환층의 선형식을 계산한다.\n",
        "    for x in seq:\n",
        "      z1=np.dot(x, self.w1x) + np.dot(self.h[-1], self.w1h) + self.b1\n",
        "      h=np.tanh(z1)                    # 활성화 함수 적용\n",
        "      self.h.append(h)                 # 역전파를 위한 은닉 상태 저장\n",
        "      z2=np.dot(h, self.w2) + self.b2  # 출력층의 선형식 계산\n",
        "    return z2\n",
        "\n",
        "  def backprop(self, x, err):\n",
        "    m=len(x) # 샘플 개수\n",
        "\n",
        "    # 출력층의 가중치와 절편에 대한 그레디언트 계산\n",
        "    w2_grad = np.dot(self.h[-1].T, err) / m\n",
        "    b2_grad = np.sum(err) / m\n",
        "    # 배치 차원과 타임 스텝 차원을 바꾼다.\n",
        "    seq = np.swapaxes(x, 0, 1)\n",
        "\n",
        "    w1h_grad = w1x_grad = b1_grad = 0 # 초기 0지정\n",
        "    # 셀 직전까지 그레디언트 계산 \n",
        "    err_to_cell = np.dot(err, self.w2.T)*(1-self.h[-1]**2)\n",
        "    # 모든 타임 스텝 거슬러 가며 그레디언트 전파\n",
        "    # 열 번째 까지 진행, h의 마지막 항목은 err_to_cell 정의에 사용하였으므로 제외.\n",
        "    # [A:B:C]코드는 indexA부터 indexB까지 C간격으로 만드는 코드\n",
        "    for x, h in zip(seq[::-1][:10], self.h[:-1][::-1][:10]):\n",
        "      w1h_grad += np.dot(h.T, err_to_cell)\n",
        "      w1x_grad += np.dot(x.T, err_to_cell)\n",
        "      b1_grad += np.sum(err_to_cell, axis=0) # 행 방향으로의 합 계산\n",
        "      #이전 타임 스텝의 셀 직전까지 그레디언트 계산\n",
        "      err_to_cell = np.dot(err_to_cell, self.w1h)*(1-h**2)\n",
        "\n",
        "    w1h_grad /= m\n",
        "    w1x_grad /= m\n",
        "    b1_grad /= m\n",
        "  \n",
        "    return w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "    a = 1 / (1+np.exp(-z))  # 시그모이드 계산\n",
        "    return a\n",
        "\n",
        "  def init_weights(self, n_features, n_classes):\n",
        "    orth_init = tf.initializers.Orthogonal() # 텐서플로가 제공하는 가중치 초기화 클래스 및 직교행렬 초기화 클래스\n",
        "    glorot_init = tf.initializers.GlorotUniform()\n",
        "\n",
        "    self.w1h = orth_init((self.n_cells, self.n_cells)).numpy() # 가중치 값이 초기화된 텐서를 넘파이 배열로 변환하여 가중치 변수에 저장 \n",
        "    self.w1x = glorot_init((n_features, self.n_cells)).numpy()\n",
        "    self.b1 = np.zeros(self.n_cells)\n",
        "    self.w2 = glorot_init((self.n_cells, n_classes)).numpy()\n",
        "    self.b2 = np.zeros(n_classes)\n",
        "\n",
        "  def fit(self, x, y, epochs=100, x_val=None, y_val=None): # x는 훈련 데이터, y는 타깃 데이터\n",
        "    y=y.reshape(-1,1)\n",
        "    y_val=y_val.reshape(-1,1)\n",
        "    np.random.seed(42)\n",
        "    self.init_weights(x.shape[2], y.shape[1])  # 은닉층과 출력층의 가중치 초기화\n",
        "    # epochs만큼 반복한다.\n",
        "    for i in range(epochs):\n",
        "      print(\"에포크\", i, end=' ')\n",
        "      # 제너레이터 함수에서 반환한 미니배치 순환한다.\n",
        "      batch_losses=[]\n",
        "      for x_batch, y_batch in self.gen_batch(x,y):\n",
        "        print('.', end='')\n",
        "        a=self.training(x_batch, y_batch)\n",
        "        # 안전한 로그 계산을 위해 클리핑한다.\n",
        "        a=np.clip(a, 1e-10, 1-1e-10)\n",
        "        # 로그 손실과 규제 손실을 더하여 리스트에 추가한다.\n",
        "        loss=np.mean(-(y_batch*np.log(a) + (1-y_batch)*np.log(1-a)))\n",
        "        batch_losses.append(loss)\n",
        "      print()\n",
        "      self.losses.append(np.mean(batch_losses))\n",
        "      # 검증 세트에 대한 손실 계산\n",
        "      self.update_val_loss(x_val, y_val)\n",
        "\n",
        "  # 미니 배치 제너레이터 함수\n",
        "  def gen_batch(self, x, y):\n",
        "    length = len(x)\n",
        "    bins = length // self.batch_size # 미니 배치 횟수\n",
        "    if length % self.batch_size:\n",
        "      bins += 1                      # 나누어 떨어지지 않을 때 배치 횟수 1추가\n",
        "    indexes=np.random.permutation(np.arange(len(x))) # 인덱스 섞기\n",
        "    x = x[indexes]\n",
        "    y = y[indexes]\n",
        "    for i in range(bins):\n",
        "      start = self.batch_size*i\n",
        "      end = self.batch_size*(i+1)               \n",
        "      yield x[start:end], y[start:end]  # batch_size만큼 슬라이싱하여 반환\n",
        "\n",
        "  def training(self, x, y):\n",
        "    m=len(x)          # 샘플 개수 저장\n",
        "    z=self.forpass(x) # 정방향 계산 수행\n",
        "    a=self.sigmoid(z) # 활성화 함수 저장\n",
        "    err=-(y-a)        # 오차 계산\n",
        "    # 오차를 역전파하여 그레디언트 계산\n",
        "    w1h_grad, w1x_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)\n",
        "    # 셀의 가중치와 절편 업데이트\n",
        "    self.w1h -= self.lr*w1h_grad\n",
        "    self.w1x -= self.lr*w1x_grad\n",
        "    self.b1 -= self.lr*b1_grad\n",
        "    # 출력층의 가중치와 절편 업데이트\n",
        "    self.w2 -= self.lr*w2_grad\n",
        "    self.b2 -= self.lr*b2_grad\n",
        "    return a\n",
        "\n",
        "  def predict(self, x):\n",
        "    z=self.forpass(x)  # 정방향 계산 수행\n",
        "    return z>0         # 스텝 함수 적용\n",
        "\n",
        "  def score(self, x, y):\n",
        "    # 예측과 타깃 열 벡터를 비교하여 True의 비율 반환\n",
        "    return np.mean(self.predict(x)==y.reshape(-1,1))\n",
        "\n",
        "  def update_val_loss(self, x_val, y_val):\n",
        "    z=self.forpass(x_val)         # 정방향 계산 수행\n",
        "    a=self.sigmoid(z)             # 활성화 함수 적용\n",
        "    a=np.clip(a, 1e-10, 1-1e-10)  # 출력값 클리핑\n",
        "    val_loss=np.mean(-(y_val*np.log(a) + (1-y_val)*np.log(1-a)))\n",
        "    self.val_losses.append(val_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1iB5JP4qxbV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "> ### 순환 신경망 모델 훈련시키기\n",
        "##### 준비한 IDMB 데이터 세트에 RecurrentNetwork 클래스를 적용해 보자.\n",
        "\n",
        "##### 셀 개수는 32개, 배치 크기는 32개, 학습률은 0.01, 에포크 횟수는 20을 사용한다. 이런 값을 포함해 TBPTT를 위한 타임 스텝 횟수는 모두 하이퍼 파라미터이다. 데이터 세트에 따라 반복적인 실험을 통해 적절한 값을 찾아야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0E-J9FYnYWE",
        "colab_type": "code",
        "outputId": "aeab08ce-aa76-43c7-c876-7285db961995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "rn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)\n",
        "\n",
        "rn.fit(x_train_onehot, y_train, epochs=20, x_val=x_val_onehot, y_val=y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크 0 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 1 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 2 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 3 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 4 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 5 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 6 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 7 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 8 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 9 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 10 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 11 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 12 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 13 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 14 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 15 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 16 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 17 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 18 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 19 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaSeytednYYY",
        "colab_type": "code",
        "outputId": "ce1abd1a-38bc-4598-ac9d-ecd192002b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rn.losses, color='red')\n",
        "plt.plot(rn.val_losses, color='green')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziTU3TY5nYa1",
        "colab_type": "code",
        "outputId": "c675c7b7-294b-4a63-849f-40cf8aa27684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "rn.score(x_val_onehot, y_val) # 검증 세트 정확도 평가"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.628"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75YXl5h2okGx",
        "colab_type": "code",
        "outputId": "526d378a-52f4-478e-b70f-8ae9eadc40f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# epochs=10으로 재설정\n",
        "\n",
        "rn = RecurrentNetwork(n_cells=32, batch_size=32, learning_rate=0.01)\n",
        "\n",
        "rn.fit(x_train_onehot, y_train, epochs=10, x_val=x_val_onehot, y_val=y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "에포크 0 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 1 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 2 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 3 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 4 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 5 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 6 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 7 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 8 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
            "에포크 9 .................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXksFQzPolKT",
        "colab_type": "code",
        "outputId": "193f77c4-d9dc-4eef-cf7d-8af590ff4e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "rn.score(x_val_onehot, y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6748"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK3YMuMTq1bm",
        "colab_type": "text"
      },
      "source": [
        "##### 영화 리뷰가 긍정인지 부정인지를 무작위로 예측하는 확률(50%)보다는 좋은 성능이 나왔다. 하지만 실전에 투입하기에는 조금 아쉬운 성능이다. 다음 절에서 텐서플로를 사용해 여러 고급 기술을 사용한 순환 신경망을 만들어 볼 것이다."
      ]
    }
  ]
}