{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforce ch2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBfhAcp8a5iSLp5wDcV2Iw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinukki/KJU/blob/master/Reinforce_ch2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOx85tkVnkLJ",
        "colab_type": "text"
      },
      "source": [
        "# Ch.2 강화학습 개념\n",
        "---\n",
        "\n",
        "### 강화학습의 5요소\n",
        " * 에이전트(agent) : 의사 결정자\n",
        " * 환경(environment) : 시스템\n",
        " * 행동(action)\n",
        " * 보상(reward)\n",
        " * 관측(observation)\n",
        "\n",
        " <br>\n",
        "\n",
        "### 강화학습은 아래의 프로세스가 반복된다.\n",
        "   1. 에이전트는 환경의 변화를 표현하는 상태(state), $x_t$를 관측 \n",
        "   2. 일정한 정책(policy) 하에 불연속적/연속적인 값으로 된 행동($u_t$)을 선택(의사 결정)하여 환경에 인가\n",
        "   3. 행동에 의해 환경의 상태는 다음 상태($x_{t+1}$)로 전환(상태 천이)됨<br>\n",
        "   <i>(이 때 환경에 의해 의사 결정 성과를 평가하는 보상($r_t$)이 주어짐 )</i>\n",
        "   4. 전환된 환경의 상태를 바탕으로 다시 에이전트는 새로운 행동 선택\n",
        "   5. 환경으로부터 주어지는 보상을 통해 장기적 성과를 계산하고 정책을 개선\n",
        "\n",
        "![1](https://user-images.githubusercontent.com/52277776/82690925-0ae58f00-9c98-11ea-975d-ffece1d13735.jpg)\n",
        "\n",
        "#### 위와 같은 프로세스를 수학적으로 모델링한 것이 마르코스 결정 프로세스<b>(MDP, Markov decision process)</b> 이다.\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N27-BJcEwBqS",
        "colab_type": "text"
      },
      "source": [
        "## MDP\n",
        "#### 상태($x_t$), 상태천이 확률밀도함수(p)와 행동($u_t$), 보상함수(r($x_t, u_t$))로 이루어진 확률 프로세스\n",
        "\n",
        "![2](https://user-images.githubusercontent.com/52277776/82690936-0faa4300-9c98-11ea-8622-fbc94251a013.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### <b>상태천이 확률밀도함수(환경의 수학적 모델)</b>는 변수의 형태에 따라 아래와 같이 나타난다. <br>또한, 미래의 상태가 과거의 상태와 행동에 관계없이 <b>현재 상태와 행동에만</b> 영향을 받는다.(마르코프 시퀀스)\n",
        "$$연속공간 \\rightarrow p(x_{t+1} \\mid x_t, u_t), 확률밀도함수$$\n",
        "$$이산공간 \\rightarrow P(x_{t+1} \\mid x_t, u_t), 확률$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### MDP 문제는 누적보상의 최대화를 위해 각 상태에서 어떤 행동을 취할 것인가를 나타내는 조건부 확률밀도함수를 구하는 것이다. 정의에 의해 확률적 정책임을 알 수 있다.\n",
        "$$정책, \\pi(u_t \\mid x_t) =p(u_t \\mid x_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### MDP에서의 운동은 다음과 같다.\n",
        "1. 상태변수 $x_0$에서 어떤 정책 $\\pi$에 의해 행동 $u_0$가 확률적으로  선택된다.\n",
        "2. 상태전이 확률$P(x_{1}\\mid x_0,u_0)$에 의해 $x_1$으로 이동한다. (<i>이때, 보상 $r(x_0,u_0)$이 주어진다.</i> )\n",
        "3. 이 과정이 반복되어 상태변수, 행동, 보상의 순서로 전개된다. 이 때 형성되는 궤적 $\\tau$는 상태변수와 행동의 연속적 시퀀스로 구성된다.\n",
        "\n",
        "$$\\tau = (x_0,u_0,x_1,u_1,\\ldots,x_T,u_T)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<i> 위와 같이 환경 모델이 상태천이 확률밀도함수로 주어지는 경우 <b>확률적 MDP</b>라고 한다.</i>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>\n",
        "\n",
        "##### <b>확정적 MDP</b>에서는 환경과 모델이 모두 확정적으로 주어지며, 보상함수도 확정적인 함수가 된다. 환경모델은 다음과 같다.\n",
        "$$x_{t+1}=f(x_t, u_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "##### 즉, 시간스텝 t에서 상태와 행동이 주어지면 $x_{t+1}$을 확정적으로 알 수 있고 이 경우, 보상은 $r(x_t, u_t)$로 주어진다. 확정적 정책은 아래와 같으며 상태변수에서 행동을 직접 계산한다.\n",
        "$$u_t=\\pi(x_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 시간스텝 t 이후 미래에 얻을 수 있는 보상의 총합을 <b>반환값(discounted return)</b>이라고 한다.\n",
        "$$G_t=\\sum_{k=t}^T  \\gamma^{k-t}r(x_k,u_k)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "##### $\\gamma \\in [0,1]$은 <b>감가율</b>이다. 값이 작을수록 보상의 가까운 미래에 가중치를 두며, 반환값의 무한대로의 발산을 막는다.\n",
        "\n",
        "<br>\n",
        "\n",
        "##### + <i>에피소드 : 어떤 정책을 시행해 $x_0 \\rightarrow u_0 \\rightarrow r(x_0,u_0)\\rightarrow \\ldots\\rightarrow x_T\\rightarrow u_T$의 순서로 전개될 때, 이러한 <b>상태변수, 행동, 보상의 시퀀스 집합,</b> 유한 구간이라면 종료시점은 t=T, 무한 구간이라면 $T \\rightarrow \\infty$ </i>\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpJWk09N5WSY",
        "colab_type": "text"
      },
      "source": [
        "## 상태가치\n",
        "#### <b>상태변수 $x_t$에서 시작</b>한 모든 가능한 궤적의 미래 보상 총합의 기댓값\n",
        "$$V^\\pi(x_t)=E_{\\tau_{u_t:u_T \\sim p(\\tau_{u_t:u_T}\\mid x_t)}}[\\sum_{k=t}^T \\gamma^{k-t}r(x_k,u_k)\\mid x_t]$$\n",
        "$$=\\int_{\\tau_{u_t:u_T}}(\\sum_{k=t}^T \\gamma^{k-t}r(x_k,u_k))p(\\tau_{u_t:u_T}\\mid x_t)d\\tau_{u_t:u_T}$$\n",
        "\n",
        "![3](https://user-images.githubusercontent.com/52277776/82690946-11740680-9c98-11ea-971a-fcb03caca4cb.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "## 행동가치\n",
        "#### <b>상태변수 $x_t$와 행동 $u_t$에서 시작</b>한 모든 가능한 궤적의 미래 보상 총합의 기댓값\n",
        "$$Q^\\pi(x_t, u_t)=E_{\\tau_{x_{t+1}:u_T \\sim p(\\tau_{x_{t+1}:u_T}\\mid x_t, u_t)}}[\\sum_{k=t}^T \\gamma^{k-t}r(x_k,u_k)\\mid x_t, u_t]$$\n",
        "$$=\\int_{\\tau_{x_{t+1}:u_T}}(\\sum_{k=t}^T \\gamma^{k-t}r(x_k,u_k))p(\\tau_{x_{t+1}:u_T}\\mid x_t, u_t)d\\tau_{x_{t+1}:u_T}$$\n",
        "\n",
        "![4](https://user-images.githubusercontent.com/52277776/82690976-1afd6e80-9c98-11ea-85b7-9c0291c8c38a.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "### 상태가치와 행동가치의 관계\n",
        "#### <b>궤도 분리</b>와 <b>베이즈 정리</b>를 이용하면, <b>상태가치</b>는 상태변수 $x_t$에서 선택 가능한 모든 행동 $u_t$에 대한 <b>행동가치의 평균값</b>이다.\n",
        "\n",
        "$$V^\\pi(x_t)=\\int_{u_t}Q^\\pi(x_t, u_t)\\pi(u_t \\mid x_t)du_t=E_{u_t \\sim \\pi(u_t \\mid x_t)}[Q^\\pi(x_t, u_t)]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf_Mit6p9lk1",
        "colab_type": "text"
      },
      "source": [
        "## 벨만 방정식\n",
        "#### <b>현재</b> 상태변수의 가치와 <b>다음 시간스텝</b>의 상태변수의 가치와의 <b>관계</b>를 나타낸다.\n",
        "<br>\n",
        "$$V^\\pi(x_t)=E_{u_t \\sim \\pi(u_t \\mid x_t)}[r(x_t,u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma V^\\pi(x_{t+1})]]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Q^\\pi(x_t)=r(x_t, u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[E_{u_{t+1} \\sim \\pi(u_{t+1} \\mid x_{t+1})}[rQ^\\pi(x_{t+1}, u_{t+1})]]$$<br>\n",
        "$$= r(x_t, u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma V^\\pi(x_{t+1})]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 여기에서도 행동가치 식의 기댓값이 상태가치 식이 된다.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ9k2xw6Cqoi",
        "colab_type": "text"
      },
      "source": [
        "## 벨만 최적 방정식\n",
        "#### 모든 정책 중 <b>상태가치 값을 최대로 만드는 정책을 적용</b>했을 때의 상태가치 함수와 행동가치 함수를 각각 <b>최적 상태가치 함수</b>와 <b>최적 행동가치 함수</b>라고 한다.\n",
        "$$V^*(x_t)=maxV^\\pi(x_t)$$\n",
        "$$Q^*(x_t, u_t)=maxQ^*\\pi(x_t,u_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 최적 상태가치 함수와 최적 행동가치 함수 및 그 관계는 다음과 같다.\n",
        "\n",
        "$$V^*(x_t)=max(r(x_t,u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma V^*(x_{t+1})])$$\n",
        "$$Q^*(x_t, u_t)=r(x_t,u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma V^*(x_{t+1})]$$\n",
        "<br>\n",
        "\n",
        "$$V^*(x_t)=maxQ^*(x_t, u_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 즉 최적 상태가치 함수는 최적 행동가치 함수의 최댓값이다. 최적 행동가치 식을 다시 쓰면 다음과 같다.\n",
        "$$Q^*(x_t, u_t)=r(x_t,u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma maxQ^*(x_{t+1}, u_{t+1})]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 위 식을 <b>벨만 최적 방정식</b>이라고 부른다. 이 식은 <b>현재</b>의 최적 가치와 <b>다음 시간스텝</b>의 최적 가치와의 <b>관계</b>를 나타내준다.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 상태가치 값을 최대로 만드는 정책을 최적 정책(optimal policy)이라고 한다.\n",
        "#### 여기서 argmax는 뒤의 항을 최대로 하는 $u_t$를 의미한다.\n",
        "$$\\pi^*(u_t \\mid x_t)=argmax(r(x_t, u_t)+E_{x_{t+1} \\sim p(x_{t+1} \\mid x_t,u_t)}[\\gamma V^*(x_{t+1})])$$$$=argmaxQ^*(x_t, u_t)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvsqA-hZIK9F",
        "colab_type": "text"
      },
      "source": [
        "## 강화학습 방법\n",
        "#### 공통점\n",
        "\n",
        "![KakaoTalk_20200527_012844232](https://user-images.githubusercontent.com/52277776/82925965-72158300-9fb9-11ea-9fa0-be5904fe237a.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 가치기반 강화학습 : 가치함수 추정 후(보통 행동가치 함수) 최대의 보상 계산<br> (각 상태에서 행동가치 함수를 최대화하는 행동을 선택, 최적 정책 유도)\n",
        "\n",
        "![2 10](https://user-images.githubusercontent.com/53015968/81430090-d226bf80-9199-11ea-8652-e6b5b5dce33c.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 정책 그래디언트 : 보상의 기댓값을 최대로 하기 위해 정책 $\\pi_\\theta (u_t \\mid x_t$)를 최적화하는 정책 파라미터 $\\theta$계산<br>(보통 액터-크리틱 구조 사용)\n",
        "\n",
        "![2 11](https://user-images.githubusercontent.com/53015968/81430093-d357ec80-9199-11ea-9e30-d9dc1fda779a.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "#### 모델 기반 강화학습 : 환경 모델 추정(간단하고 효율적 $\\rightarrow$ 경쟁력), 정책 개선은 환경 모델 정의 방식에 따라 여러 가지 방법 사용\n",
        "\n",
        "![2 12](https://user-images.githubusercontent.com/53015968/81430098-d4891980-9199-11ea-83e2-6cd439c72da4.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3D3R2lhlzF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}