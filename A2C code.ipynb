{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOdTh7kHwoYXdud53RLlAXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinukki/KJU/blob/master/A2C%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C0AkfIBaY0W",
        "colab_type": "text"
      },
      "source": [
        "## a2c_actor.py\n",
        "---\n",
        "### A2C 액터 신경망을 설계한 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkKWfP0eaWNR",
        "colab_type": "code",
        "outputId": "ae2d1963-7f10-4dc1-ae59-a75ac8719c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 필요한 패키지 임포트\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Lambda\n",
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlVkySVHasi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(object):\n",
        "  \"\"\"\n",
        "      A2C의 액터 신경망\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, action_dim, action_bound, learning_rate):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.action_bound = action_bound\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    ## 표준편차의 최솟값과 최댓값 설정\n",
        "    self.std_bound = [1e-2, 1.0]\n",
        "\n",
        "    ## 액터 신경망 생성\n",
        "    self.model, self.theta, self.states = self.build_network()\n",
        "\n",
        "    ## 손실함수와 그래디언트\n",
        "    self.actions = tf.placeholder(tf.float32, [None, self.action_dim])\n",
        "    self.advantages = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "    mu_a, std_a = self.model.output\n",
        "    log_policy_pdf = self.log_pdf(mu_a, std_a, self.actions)\n",
        "\n",
        "    loss_policy = log_policy_pdf * self.advantages\n",
        "    loss = tf.reduce_sum(-loss_policy)\n",
        "    dj_dtheta = tf.gradients(loss, self.theta)\n",
        "    grads = zip(dj_dtheta, self.theta)\n",
        "    self.actor_optimizer = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(grads)\n",
        "\n",
        "    ## 액터 신경망\n",
        "    def build_network(self):\n",
        "      state_input = Input((self.state_dim,))\n",
        "      h1 = Dense(64, activation='relu')(state_input)\n",
        "      h2 = Dense(32, activation='relu')(h1)\n",
        "      h3 = Dense(16, activation='relu')(h2)\n",
        "      out_mu = Dense(self.action_dim, activation='tanh')(h3)\n",
        "      std_output = Dense(self.action_dim, activation='softplus')(h3)\n",
        "\n",
        "    ## 평균값을 [-action_bound, action_bound] 범위로 조정\n",
        "    mu_output = Lambda(lambda x: x*self.action_bound)(out_mu)\n",
        "    model = Model(state_input, [mu_output, std_output])\n",
        "    model.summary()\n",
        "    return model, model.trainable_weights, state_input\n",
        "\n",
        "    ## 로그-정책 확률밀도 함수\n",
        "    def log_pdf(self, mu, std, action):\n",
        "      std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
        "      var = std**2\n",
        "      log_policy_pdf = -0.5*(action-mu)**2/var - 0.5*tf.log(var*2*np.pi)\n",
        "      return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
        "\n",
        "    ## 액터 신경망 출력에서 확률적으로 행동을 추출\n",
        "    def get_action(self, state):\n",
        "      mu_a, std_a = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
        "      mu_a = mu_a[0]\n",
        "      std_a = std_a[0]\n",
        "      std_a = np.clip(std_a, self.std_bound[0], self.std_bound[1])\n",
        "      action = np.random.normal(mu_a, std_a, size=self.action_dim)\n",
        "      return action\n",
        "\n",
        "    ## 액터 신경망에서 평균값 계산\n",
        "    def predict(self, state):\n",
        "      mu_a, _ = self.model.predict(np.reshape(state, [1, self.state_dim]))\n",
        "      return mu_a[0]\n",
        "\n",
        "    ## 액터 신경망 학습\n",
        "    def train(self, states, actions, advantages):\n",
        "      slef.actor_optimizer\n",
        "\n",
        "    ## 액터 신경망 파라미터 저장\n",
        "    def save_weights(self, path):\n",
        "      self.model.save_weights(path)\n",
        "\n",
        "    ## 액터 신경망 파라미터 로드\n",
        "    def load_weights(self, path):\n",
        "      self.model.load_weights(paht + 'pendulum_actor.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJkYoh1j-zin",
        "colab_type": "text"
      },
      "source": [
        "## a2c_critic.py\n",
        "---\n",
        "### A2C 크리틱 신경망을 설계한 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qw1rhoqW3wC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(object):\n",
        "  \"\"\"\n",
        "      A2C의 크리틱 신경망\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, action_dim, leartning_rate):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "    ## 크리틱 신경망 생성\n",
        "    self.model, self.states = self.build_network()\n",
        "\n",
        "    ## 학습 방법 설정\n",
        "    self.model.compile(optimizer=Adam(self.learning_rate), loss='mse')\n",
        "\n",
        "    ## 크리틱 신경망\n",
        "    def builde_network(self):\n",
        "      state_input = Input((self.state_dim,))\n",
        "      h1 = Dense(64, activation='relu')(state_input)\n",
        "      h2 = Dense(32, activation='relu')(h1)\n",
        "      h3 = Dense(16, activation='relu')(h2)\n",
        "      v_output = Dense(1, activation='linear')(h3)\n",
        "      model = Model(state_input, v_output)\n",
        "      model.summary()\n",
        "      return model, state_input\n",
        "\n",
        "    ## 배치 데이터(batch data)로 크리틱 신경망 업데이트\n",
        "    def train_on_batch(self, states, td_targets):\n",
        "      return self.model.train_on_batch(states, td_targets)\n",
        "\n",
        "    ## 크리틱 신경망 파라미터 저장\n",
        "    def save_weights(self, path):\n",
        "      self.model.save_weights(path)\n",
        "\n",
        "    ## 크리틱 신경망 파라미터 로드\n",
        "    def load_weights(self, path):\n",
        "      self.model.load.weights(path + 'pendulum_critic.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiQE8WJiBMU2",
        "colab_type": "text"
      },
      "source": [
        "## a2c_agent.py\n",
        "---\n",
        "### A2C 에이전트를 학습하고 평가하는 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDuBHzBCCZ0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 추가적으로 필요한 패키지 임포트\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS5uw10WCnlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class A2Cagent(object):\n",
        "  def __init__(self, env):\n",
        "\n",
        "    ## 하이퍼파라미터\n",
        "    self.GAMMA = 0.95\n",
        "    self.BATCH_SIZE = 32\n",
        "    self.ACTOR_LEARNING_RATE = 0.0001\n",
        "    self.CRITIC_LEARNING_RATE = 0.001\n",
        "\n",
        "    ## 환경\n",
        "    self.env = env\n",
        "    ## 상태변수 차원(dimensein)\n",
        "    self.state_dim = env.observation_space.shape[0]\n",
        "    ## 행동 차원(dimension)\n",
        "    self.action_dim = env.action_space.shape[0]\n",
        "    ## 행동의 최대 ㅋ기\n",
        "    self.action_bound = env.action_space.high[0]\n",
        "\n",
        "    ## 액터 신경망 및 크리틱 신경망 생성\n",
        "    self.actor = Actor(self.self.state_dim, self.action_dim, self.action_dim, self.action_bound, self.ACTOR_LEARNING_RATE)\n",
        "    self.critic = Critic(self.state_dim, self.action_dim, self.CRITIC_LEARNING_RATE)\n",
        "\n",
        "    ## 그래디언트 계산을 위한 초기화\n",
        "    tf.global_variables_initializer()\n",
        "\n",
        "    ## 에피소드에서 얻은 총 보상값을 저장하기 위한 변수\n",
        "    self.save_epi_reward = []\n",
        "\n",
        "  ## 어드밴티지와 TD타깃 계산\n",
        "  def advantage_td_target(self, reward, v_value, next_v_value, done):\n",
        "    if done :\n",
        "      y_k = save_epi_reward\n",
        "      advantage = y_k - v_value\n",
        "    else :\n",
        "      y_k = reward + self.GAMMA * next_v_value\n",
        "      advantage = y_k - v_value\n",
        "    return advantage, y_k\n",
        "\n",
        "    ## 배치에 저장된 데이터 추출\n",
        "    def unpack_batch(self, batch):\n",
        "      unpack = batch[0]\n",
        "      for idx in range(len(batch)-1):\n",
        "        unpack = np.append(unpack, batch[idx+1], axis=0)\n",
        "      return unpack\n",
        "\n",
        "    ## 에이전트 학습\n",
        "    def train(self, max_episode_num):\n",
        "      ## 에피소드마다 다음을 반복\n",
        "      for ep in range(int(max_episode_num)):\n",
        "        ## 배치 초기화\n",
        "        batch_state, batch_action, batch_td_target, batch_advantage = [], [], [], []\n",
        "        ## 에피소드 초기화\n",
        "        time, episode_reward, done=0, 0, False\n",
        "        ## 환경 초기화 및 초기 상태 관측\n",
        "        state = self.env.reset()\n",
        "        while not done:\n",
        "          ## 환경 가시화\n",
        "          self.env.render()\n",
        "          ## 행동 추출\n",
        "          action = self.actor.get_action(state)\n",
        "          ## 행동 범위 클리핑\n",
        "          action = np.clip(action, -self.action_bound, self.action_bound)\n",
        "          ## 다음 상태, 보상 관측\n",
        "          next_staet, reward, done, _ = self.env.step(action)\n",
        "          ## shape 변환\n",
        "          state = np.reshape(state, [1, self.state_dim])\n",
        "          next_state = np.reshape(next_state, [1, self.state_dim])\n",
        "          action = np.reshape(action, [1, self.action_dim])\n",
        "          reward = np.reshape(reward, [1,1])\n",
        "          ## 상태가치 계산\n",
        "          v_value = self.critic.model.predict(state)\n",
        "          next_v_value = self.critic.model.predict(next_state)\n",
        "          train_reward = (reaward+8)/8\n",
        "          advantage, y_i = self.advantage_td_target(train_reward, v_value, next_v_value, done)\n",
        "          ## 배치에 저장\n",
        "          batch_state.append(state)\n",
        "          batch_action.append(action)\n",
        "          batch_td_target.append(y_i)\n",
        "          batch_advantage.append(advantage)\n",
        "\n",
        "          ## 배치가 채워질 때까지 학습하지 않고 저장만 계속\n",
        "          if len(batch_state) < self.BATCH_SIZE:\n",
        "            ## 상태 업데이트\n",
        "            state = next_state[0]\n",
        "            episode_reward += reward[0]\n",
        "            time += 1\n",
        "            continue\n",
        "\n",
        "          ## 배치가 채워지면 학습 진행, 배치에서 데이터 추출\n",
        "          states = self.unpack_batch(batch_state)\n",
        "          actions = self.unpack_batch(batch_action)\n",
        "          td_targets = self.unpack_batch(batch_td_target)\n",
        "          advantages = self.unpack.batch(batch_advantage)\n",
        "          ## 배치 비움\n",
        "          batch_state, batch_action, batch_td_target, batch_advantage = [], [], [], []\n",
        "\n",
        "          ## 크리틱 신경망 업데이트\n",
        "          self.critic.train_on_batch(states, td_targets)\n",
        "          ## 액터 신경망 업데이트\n",
        "          self.actor.train(states, actions, advantages)\n",
        "\n",
        "          ## 상태 업데이트\n",
        "          state = next_state[0]\n",
        "          episode_reward += reward[0]\n",
        "          time += 1\n",
        "\n",
        "        ## 에피소드마다 결과 보상값 출력\n",
        "        print('Episode: ', ep+1, 'Time: ', time, 'Reward: ', episode_reward)\n",
        "        self.save_epi_reward.appen(episode_reward)\n",
        "\n",
        "        ## 에피소드 10번마다 신경망 파라미터를 파일에 저장\n",
        "        if ep % 10 ==0 :\n",
        "          self.actor.save_weights(\"./save_weights/pendulum_actor.h5\")\n",
        "          self.critic.save_weights(\"./save_weights/pendulum_critic.h5\")\n",
        "\n",
        "      np.savetxt('./save_weights/pendulum_epi_reward.txt', self.save_epi_reward)\n",
        "\n",
        "    def plot_result(self):\n",
        "      plt.plot(self.save_epi_reward)\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI1Di3y3p9sw",
        "colab_type": "text"
      },
      "source": [
        "## a2c_main.py\n",
        "---\n",
        "### A2C 에이전트를 학습하고 결과를 도시하는 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf5wB1gjqEzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 추가로 필요한 패키지 임포트\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaenDTRLqSkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  max_episode_num = 100 ## 최대 에피소드 설정\n",
        "  env_name = 'Pendulum-v0'\n",
        "  env = gym.make(env_name) ## 환경으로 OpenAI Gym이 pendulum-v0 설정\n",
        "  agent = A2Cagent(env) ## A2C 에이전트 객체\n",
        "\n",
        "  ## 학습 진행\n",
        "  agent.train(max_episode_num)\n",
        "\n",
        "  ## 학습 결과 도시\n",
        "  agent.plot_result()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkw-LMUrBMU",
        "colab_type": "text"
      },
      "source": [
        "## a2c_load_play.py\n",
        "---\n",
        "### 학습된 신경망 파라미터를 가져와서 에이전트를 실행시키는 파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13VZ7KAgrIAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "  env_name = 'Pendulum-v0'\n",
        "  env = gym.make(env_name) ## 환경으로 OpenAI Gym이 pendulum-v0 설정\n",
        "  agent = A2Cagent(env) ## A2C 에이전트 객체\n",
        "\n",
        "  agent.actor.load_weights('./save_weights/') ## 액터 신경망 파라미터 가져옴\n",
        "  agent.critic.load_weights('./save_weights/') ## 크리틱 신경망 파라미터 가져옴\n",
        "\n",
        "  time = 0\n",
        "  state = env.reset() ## 환경을 초기화하고 초기 상태 관측\n",
        "  \n",
        "  while True :\n",
        "    env.render()\n",
        "    action = agent.actor.predict(state) ## 행동계산\n",
        "    state, reward, done, _ = env.step(action) ## 환경으로부터 다음 상태, 보상 받음\n",
        "    time += 1\n",
        "\n",
        "    print('Time: ', time, 'Reward: ', reward)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}