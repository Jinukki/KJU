{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch9-1(2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinukki/KJU/blob/master/Ch9_1(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8mrIkrJLIyM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "> ### 순환 신경망의 역방향 계산을 알아봅시다.\n",
        "##### 먼저 가중치 $W_2$에 대한 손실 함수의 도함수를 알아보자.\n",
        "\n",
        "![10](https://user-images.githubusercontent.com/52277776/68992875-b90d1800-08b4-11ea-8773-63ce698bebf4.jpg)\n",
        "\n",
        "##### 출력층의 가중치 $W_2$에 대해 손실 함수 L을 미분하기 위해 연쇄 법칙을 적용하여 다음과 같은 도함수를 얻을 수 있다.\n",
        "<br>\n",
        "$$\\frac{\\partial L}{\\partial W_2}=\\frac{\\partial L}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_2}=H^{T}(-(Y-A_2))$$\n",
        "<br>\n",
        "$$\\frac{\\partial L}{\\partial b_2}=\\frac{\\partial L}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial b_2}=1^{T}(-(Y-A_2))$$\n",
        "\n",
        "##### 앞의 6장에서의 완전 연결 신경망 은닉층의 출력 $A_1$을 셀의 출력 $H$로 바꾼 것 외에는 동일하다.\n",
        "\n",
        "##### $H$에 대한 $Z_2$의 도함수는 다음과 같이 가중치 $W_2$가 된다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_2}{\\partial H}=\\frac{\\partial}{\\partial H}(HW_{2}+b_2)=W_2$$\n",
        "\n",
        "##### $Z_1$에 대한 $H$의 도함수를 구하기 위해서는 tanh 함수의 도함수를 유도해야 한다.\n",
        "<br>\n",
        "$$ tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $$\n",
        "<br>\n",
        "$$\\frac{dtatnh(x)}{dx}=\\frac{(e^{x}+e^{-x})\\frac{d}{dx}(e^{x}-e^{-x})-(e^{x}-e^{-x})\\frac{d}{dx}(e^{x}+e^{-x})}{(e^{x}+e^{-x})^2}$$\n",
        "<br>\n",
        "$$=\\frac{(e^{x}+e^{-x})^2-(e^{x}-e^{-x})^2}{(e^{x}+e^{-x})^2}=1-(\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}})^2=1-tanh^2(x)$$\n",
        "\n",
        "##### 따라서 $tanh(x)$의 도함수는 다음과 같다.\n",
        "<br>\n",
        "$$1-tanh^2(x)$$ \n",
        "<br>\n",
        "$$\\frac{\\partial H}{\\partial Z_1}=\\frac{\\partial}{\\partial Z_1}tanh(Z_1)=1-tanh^2(Z_1)=1-H^2$$\n",
        "\n",
        "##### 지금까지의 도함수를 표시하면 다음과 같다.\n",
        "\n",
        "![11](https://user-images.githubusercontent.com/52277776/68992878-bca09f00-08b4-11ea-810a-1376042e6f93.jpg)\n",
        "\n",
        "##### 또한 위를 바탕으로 $Z_1$에 대한 손실 함수 L의 도함수는 연쇄 법칙을 적용하여 다음과 같이 쓸 수 있다.\n",
        "<br>\n",
        "$$\\frac{\\partial L}{\\partial Z_2}=\\frac{\\partial L}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial H}\\frac{\\partial H}{\\partial Z_1}=-(Y-A_2)W^T\\odot(1-H^2)$$\n",
        "\n",
        "---\n",
        "\n",
        "##### 이제 가중치 $W_{1h}$의 업데이트에 사용할 $W_{1h}$에 대한 $Z_1$의 도함수(그래디언트)를 구해보자. 여기서 추가적으로 고려해야할 점은 이전 타임 스텝의 은닉 상태 $H_p$도 $W_{1h}$를 사용하므로 상수로 취급할 수 없다는 점이다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=\\frac{\\partial}{\\partial W_{1h}}(XW_{1x}+H_{p}W_{1h}+b_1)=H_p$$\n",
        "<br>\n",
        "\n",
        "##### 다음은 현재와 이전 타임 스텝의 은닉 상태를 포함한 순환 신경망의 계산 과정을 동시에 나타낸 것이다.\n",
        "\n",
        "![12](https://user-images.githubusercontent.com/52277776/68992879-bf02f900-08b4-11ea-9d72-a3876947c51b.jpg)\n",
        "\n",
        "##### 그림을 보면 이전 타임 스텝 이전 은닉 상태 $H_p$도 $W_{1h}$에 의해 영향을 받고 있다는 것을 쉽게 알 수 있다. $H_p$는 다음과 같은 식으로 계산된다.\n",
        "<br>\n",
        "$$Z_{1p}=X_{p}W_{1x}+H_{pp}W_{1h}+b_1$$\n",
        "$$H_{p}=tanh(Z_{1p})$$\n",
        "<br>\n",
        "\n",
        "##### $H_p$는 이전 타임 스텝의 $X_p$와 두 타임 스텝 이전의 은닉 상태 $H_{pp}$를 사용해 계산한다. 중요한 점은 타임 스텝마다 같은 가중치를 사용한다는 점이다. 가중치는 훈련 데이터에 있는 시퀀스를 차례대로 모두 진행한 후 마지막에 업데이트 된다. $H_p$rk $W_{1h}$의 함수이므로 도함수를 다시 계산해 보자.\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=\\frac{\\partial}{\\partial W_{1h}}(XW_{1x}+H_{p}W_{1h}+b_1)=\\frac{\\partial}{\\partial W_{1h}}(H_{p}W_{1h})=H_{p}\\frac{\\partial W_{1h}}{\\partial W_{1h}}+W_{1h}\\frac{\\partial H_{p}}{\\partial W_{1h}}=H_p+W_{1h}\\frac{\\partial H_{p}}{\\partial W_{1h}}$$\n",
        "<br>\n",
        "\n",
        "##### $\\frac{\\partial H_{p}}{\\partial W_{1h}}$ 에 대해 연쇄 법칙을 적용하여 더 자세히 나타내 보자.\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=H_{p}+W_{1h}\\frac{\\partial H_{p}}{\\partial Z_{1p}}\\frac{\\partial Z_{1p}}{\\partial W_{1h}}$$\n",
        "<br>\n",
        "\n",
        "##### 앞에서 구한 tanh의 미분을 통해 $\\frac{\\partial H_{p}}{\\partial Z_{1p}}$를 다음과 같이 나타낼 수 있다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=H_{p}+W_{1h}\\odot(1-{H_{p}}^2)\\frac{\\partial Z_{1p}}{\\partial W_{1h}}$$\n",
        "<br>\n",
        "\n",
        "##### 앞에서 보았듯이 $Z_{1p}$는 $W_{1h}$의 함수이다. 앞의 방식을 반복하여 다음과 같은 식을 얻을 수 있다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=H_{p}+W_{1h}\\odot(1-H_{p}^2)(H_{pp}+W_{1h}\\odot(1-H_{pp}^2)\\frac{\\partial Z_{1pp}}{\\partial W_{1h}}$$\n",
        "<br>\n",
        "\n",
        "##### 결과를 보니 $W_{1h}$에 대해 $Z_1$의 미분이 반복되는 것을 볼 수 있다. 이런 식으로 순환 신경망에 주입한 모든 타임 스텝을 거슬러 올라갈 때까지 반복된다. 이를 시간을 거슬러 역전파(Backpropagation Through Time;BPTT)된다고 말한다. 특히 순환 신경망의 역전파 구현을 위해서는 각 타임 스텝마다 셀의 출력을 모두 기억해서 가지고 있어야 한다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1h}}=H_{p}+H_{pp}W_{1h}\\odot(1-H_{p}^2)(H_{ppp}W_{1h}\\odot(1-H_{p}^2)\\odot W_{1h}\\odot(1-H_{pp}^2)+\\dots$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLwR0MXZRINO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##### 다음은 가중치 $W_{1x}$에 그레디언트를 업데이트 하기 위해 $W_{1x}$에 대한 $Z_1$의 도함수를 구해보자.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1x}}=\\frac{\\partial}{\\partial W_{1x}}(XW_{1x}+H_{p}W_{1h}+b_1)$$\n",
        "\n",
        "##### $H_p$가 $W_{1x}$의 함수임을 고려하여 전개해보자.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1x}}=X+W_{1h}\\frac{\\partial H_p}{\\partial W_{1x}}=X+W_{1h}\\frac{\\partial H_p}{\\partial Z_{1p}}\\frac{\\partial Z_{1p}}{\\partial W_{1x}}=X+W_{1h}\\odot(1-H_{p}^2)\\frac{\\partial Z_{1p}}{\\partial W_{1x}})$$\n",
        "\n",
        "##### 역시 $\\frac{\\partial Z_{1p}}{\\partial W_{1x}}$ 가 반복되며 이를 한 타임 스텝에 대해 더 전개하고 이어서 패턴을 풀어서 나타내면 아래와 같다.\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1x}}=X+W_{1h}\\odot(1-H_{p}^2)+(X+W_{1h}\\odot(1-H_{pp}^2)\\frac{\\partial Z_{1pp}}{\\partial W_{1x}}$$\n",
        "<br>\n",
        "$$\\frac{\\partial Z_1}{\\partial W_{1x}}=X+XW_{1h}\\odot(1-H_{p}^2)+XW_{1h}\\odot(1-H_{p}^2)\\odot+XW_{1h}\\odot(1-H_{pp}^2)+\\dots$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0iUZqx6VLZA",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "##### 마지막으로 절편 $b_1$에 대한 $Z_1$의 도함수를 구해보자. 마찬가지로 $H_p$가 $b_1$의 함수이므로 비슷한 전개 과정을 거친다.\n",
        "<br>\n",
        "\n",
        "$$\\frac{\\partial Z_1}{\\partial b_1}=\\frac{\\partial}{\\partial b_1}(X+W_{1x}+H_{p}W_{1h}+b_1)=W_{1h}\\frac{\\partial H_p}{\\partial b_1}+\\frac{\\partial b_1}{\\partial b_1}=1+W_{1h}\\frac{\\partial H_p}{\\partial Z_{1p}}\\frac{\\partial Z_{1p}}{\\partial b_1}$$\n",
        "<br>\n",
        "$$=1+W_{1h}\\odot(1-H_{p}^2)\\frac{\\partial Z_{1p}}{\\partial b_1}=1+W_{1h}\\odot(1-H_{p}^2)(1+W_{1h}\\odot(1-H_{pp}^2)\\frac{\\partial Z_{1pp}}{\\partial b_1}$$\n",
        "<br>\n",
        "$$=1+W_{1h}\\odot(1-H_{p}^2)+W_{1h}\\odot(1-H_{p}^2)\\odot W_{1h}\\odot(1-H_{pp}^2)+\\dots$$\n",
        "<br>\n",
        "##### 지금까지 출력층의 가중치에 대한 도함수를 구하고 그 다음 순환 셀 직전까지 역전파된 그레디언트 $\\frac{\\partial L}{\\partial Z_1}$을 구했고 마지막에 순환 셀의 가중치에 대한 $Z_1$의 도함수 $\\frac{\\partial Z_1}{\\partial W_{1h}}$,$\\frac{\\partial Z_1}{\\partial W_{1x}}$,$\\frac{\\partial Z_1}{\\partial b_1}$를 구했다. 09-2절에서는 그레디언트 $\\frac{\\partial L}{\\partial Z_1}$를 err_to_cell이라고 표현한다. 가중치 $W_{1x}$,$W_{1h}$,$b_1$에 업데이트할 최종 그레디언트는 err_to_cell과 $\\frac{\\partial Z_1}{\\partial W_{1h}}$,$\\frac{\\partial Z_1}{\\partial W_{1x}}$,$\\frac{\\partial Z_1}{\\partial b_1}$를 곱해서 구한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKGGRuXnZXj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}